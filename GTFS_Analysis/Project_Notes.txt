Link to publicly available data:
https://www.transitwiki.org/TransitWiki/index.php/Publicly-accessible_public_transportation_data

Massachussetts data:
https://www.mbta.com/developers/gtfs-realtime

See overview of Spark streaming to better understand how this process works:
https://spark.apache.org/docs/latest/streaming-programming-guide.html


Needed software:
    1) apache kafka
    2) apache spark

Python Libraries    
    1) apache sedona
    2) pyspark
    3) gtfs realtime bindings
    4) confluent kafka 
    5) requests

See Sedona's pip file for their currect versions of python libraries:
    https://github.com/apache/sedona/blob/master/python/Pipfile

The following processes need to be running before starting this process:
    1) kafka
        a) systemctl start kafka
    2) spark master
        systemctl start spark-master
    3) spark worker
        systemctl start spark-worker  
    Note:
        Can run all these at once with:  systemctl start kafka spark-worker spark-master

Step by step process:
    1. *LOOKS LIKE YOU CAN SKIP THIS STEP???* Upon opening linux on a new computer, make sure to create a kafka topic to ingest the data into.  
        a) These kafka topics dissapear eventually.
        b) Run this command to create the kafka topic:
            kafka-topics.sh --create --topic mbta-realtime --bootstrap-server localhost:9092
        
    2. Run the Kafka_Producer_v1.py python script.  
        a) This will publish the data into the apache kafka topic for ingestion into spark.
        b) Run continuously as you run the analysis

    3. Run the Bash script that also runs the Spark_Streaming_Session_Creator_v1.py python script
        a) This will create the streaming session and run the analysis with sedona.
        b) Needs to be run from cli because spark-submit must be run first to establish the streaming session
        c) Bash script for enabling spark streaming and running the analysis script:
            spark-submit --master spark://localhost:9092 --deploy-mode client --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1 /home/nate/Documents/Python_Projects/GTFS_Analysis/Spark_Streaming_Session_Creator_v1.py

Daily Notes:
    1. Might make sense to create a bash script that will also create the kafka topic before
    running the producer script (7/8). 

    2. Still can't shake the issues with registering the spatial sql functions.  
    Joined the sedona discord server and will ask them about how to do it(7/16).

    3. I think I might be okay now with the spatial functions? 
    I think the issue was the way ChatGPT was formatting the code. 
    Next step is to figure out how to parse out the geometry information from the json data (7/23).

    4. It appears that I have a dataframe with a geometry and a timestamp value. Need to run some more tests.
    Next I will add in the geofence dataframes and get geopandas set up (7/28).

    5. Have configured geopandas for use with the sedona environment. Geofences have been addeds as geopandas geodataframes.
    Have also begun to use jupyter notebook for additional visual QC. 
    Should decide about whether to convert those to sedona dataframes (7/30).    

    6. Need to fix the geometry column showing null for the values. 
    Issue is that the data has been serialized to protobuf and I need it deserialized back to json. 
    I can then proceed with analysis (09/01)

    7. Made some changes to the streaming script and it seems to be formatted better. 
    Not sure exactly why the streaming query is still returning null values (09/06)
